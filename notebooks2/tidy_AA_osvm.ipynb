{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "#Choose where to load the files from\n",
    "b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "# b_h5 = '/mnt/hdf5test/'\n",
    "# b_h5   = '/home/test_local/'\n",
    "\n",
    "pds  = {1: 'BTagCSV', 2: 'BTagMu', 3: 'Charmonium', 4:'DisplacedJet', 5: 'DoubleEG',\n",
    "        6: 'DoubleMuon', 7: 'DoubleMuonLowMass',\n",
    "       # 8: 'FSQJets', 9: 'HighMultiplicityEOF', #NOT ENOUGH DATA, NOTEBOOK FAILES\n",
    "        10: 'HTMHT', 11: 'JetHT', 12: 'MET',\n",
    "       # 13: 'MinimumBias', #NOT ENOUGH DATA\n",
    "        14: 'MuonEG', 15: 'MuOnia',\n",
    "       # 16: 'NoBPTX',\n",
    "        17: 'SingleElectron', 18: 'SingleMuon', 19: 'SinglePhoton', 20: 'Tau', 21: 'ZeroBias'\n",
    "}\n",
    "\n",
    "#Choose which PD to load\n",
    "nbr = 11\n",
    "\n",
    "bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "      \n",
    "def get_jets(bg_files, bg_jets, sig_files, sig_jets):\n",
    "    good_jets = np.empty([0,2802])\n",
    "    bad_jets  = np.empty([0,2802])\n",
    "                   # Control which time intervals files per PD to load\n",
    "    for i in range(0,len(bg_files)-4):\n",
    "        try:\n",
    "            bg_jetfile  = h5py.File(bg_files[i],'r')\n",
    "            bg_jet      = bg_jetfile[bg_jets[i]][:]\n",
    "            sig_jetfile = h5py.File(sig_files[i],'r')\n",
    "            sig_jet     = sig_jetfile[sig_jets[i]][:]\n",
    "\n",
    "            bad_jets    = np.concatenate((bad_jets, bg_jet), axis=0)\n",
    "            good_jets = np.concatenate((good_jets, sig_jet), axis=0)\n",
    "            print( \"Number of good lumis: \", len(sig_jet), \" Number of bad lumis: \", len(bg_jet)) \n",
    "\n",
    "        except OSError:\n",
    "            print(\"This Primary Dataset doesn't have \", bg_jets[i])\n",
    "    return good_jets, bad_jets\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load good and bad jets\n",
    "good_jets, bad_jets = get_jets(bg_files, bg_jets, sig_files, sig_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import setGPU  \n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Assign good jets class label 0\n",
    "df1 = pd.DataFrame(good_jets)\n",
    "# cutted_df = df1.iloc[0:1000, :]   #Temporarily to make training faster\n",
    "# df1 = cutted_df                   #Temporarily to make training faster\n",
    "df1['class'] = 0\n",
    "\n",
    "#Assign bad_jets class label  1\n",
    "df2 = pd.DataFrame(bad_jets)\n",
    "# cutted_df = df2.iloc[0:30, :]    #Temporarily to make training faster\n",
    "# df2 = cutted_df                   #Temporarily to make training faster\n",
    "df2['class'] = 1\n",
    "\n",
    "del(good_jets)\n",
    "del(bad_jets)\n",
    "#Concatenate them\n",
    "frames = [df1,df2]\n",
    "data   = pd.concat(frames)\n",
    "\n",
    "del(df1)\n",
    "del(df2)\n",
    "#Shuffle them randomly\n",
    "data = shuffle(data)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#Save labels and delete them from df not to cheat during training\n",
    "labels = data['class'].astype(int)\n",
    "del data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Normalize the data to make training better\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = standard_scaler.fit_transform(data)\n",
    "data_n = pd.DataFrame(np_scaled)\n",
    "\n",
    "# #Make training and dev set\n",
    "# X_train, X_dev = train_test_split(data_n, test_size=0.05, random_state=RANDOM_SEED)\n",
    "\n",
    "# X_train = X_train.values\n",
    "# X_dev   = X_dev.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import my_metrics\n",
    "\n",
    "#Reduce the dimensionality of data_n\n",
    "encoding_dim = 100\n",
    "input = Input(shape=(2802,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input)\n",
    "decoded = Dense(2802, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(inputs=input, outputs=decoded)\n",
    "\n",
    "encoder = Model(inputs=input, outputs=encoded)\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))\n",
    "\n",
    "#Use command \"tensorboard --logdir Graph\" to launch TensorBoard\n",
    "# tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# checkpointer = ModelCheckpoint(filepath=\"model_tidyAAosvm.h5\",\n",
    "#                                verbose=0,\n",
    "#                                save_best_only=True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy',\n",
    "                             my_metrics.mcor]\n",
    "                   )\n",
    "\n",
    "\n",
    "history = autoencoder.fit(data_n.values, data_n.values,\n",
    "                epochs=2,\n",
    "                batch_size=1000,\n",
    "#                 validation_data=(X_dev, X_dev),\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                callbacks=[#checkpointer,\n",
    "                           tensorboard]).history    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Uncomment to save the trained model (lower-dimensional representation of good_jets and bad_jets)\n",
    "\n",
    "autoencoder.save(\"model_tidyAAosvm.h5\")\n",
    "\n",
    "# autoencoder.save_weights('my_model_weights_jetC_50_300ep.h5')\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "autoencoder = load_model(\"model_tidyAAosvm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "f = plt.figure()\n",
    "plt.plot(history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Store the encoded matrix in \"encoded\". \n",
    "encoded = encoder.predict(data_n.values)\n",
    "# decoded = decoder.predict(encoded)   \n",
    "print(encoded.shape)\n",
    "new_enc = pd.DataFrame(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize data again for OneClassSVM this time\n",
    "# encoded = pd.DataFrame(encoded)\n",
    "# encoded_scaled= standard_scaler.fit_transform(encoded)\n",
    "# new_enc = pd.DataFrame(encoded_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add class labels back and split 'new_enc' to sigVals and backVals, transform them back to non-pd-dataframe\n",
    "new_enc[\"class\"] = labels\n",
    "\n",
    "sigVals  = new_enc[new_enc['class'] == 0]\n",
    "backVals = new_enc[new_enc['class'] == 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split sigVals into sigValsTrain and sigValsTest so that OneClassSVM can train on good only\n",
    "sigValsTrain, sigValsTest = train_test_split(sigVals, test_size = 0.2, random_state = 42)\n",
    "\n",
    "sigValsTrain = pd.DataFrame(sigValsTrain)\n",
    "del sigValsTrain['class']\n",
    "\n",
    "#Save labels for test signal and backvals for the roc curve\n",
    "sigValsTest = pd.DataFrame(sigValsTest)\n",
    "labels_sigValsTest = sigValsTest['class'].astype(int)\n",
    "del sigValsTest['class']\n",
    "\n",
    "backVals = pd.DataFrame(backVals)\n",
    "labels_backVals = backVals['class'].astype(int)\n",
    "del backVals['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(labels_sigValsTest.shape)\n",
    "print(labels_backVals.shape)\n",
    "kk = np.append(labels_sigValsTest,labels_backVals)\n",
    "print(kk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Train OneClassSVM model on good only.\n",
    "from sklearn.svm import *\n",
    "\n",
    "nuVal    = 0.01\n",
    "gammaVal = \"auto\"\n",
    "clf  = OneClassSVM(nu=nuVal, kernel = 'rbf', gamma = gammaVal)\n",
    "clf.fit(sigValsTrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Predict based on \"clf.fit(sigValsTrain)\"\n",
    "y_pred_train = clf.predict(sigValsTrain)\n",
    "y_pred_test = clf.predict(sigValsTest)\n",
    "y_pred_outliers = clf.predict(backVals)\n",
    "\n",
    "falseNegTrain = y_pred_train[y_pred_train == -1].size\n",
    "falseNegTest = y_pred_test[y_pred_test == -1].size\n",
    "falsePos = y_pred_outliers[y_pred_outliers == 1].size\n",
    "truePosTrain = y_pred_train[y_pred_train == 1].size\n",
    "truePosTest = y_pred_test[y_pred_test == 1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plots classification results for signal and background\n",
    "osvmArrs = []\n",
    "osvmHists = []\n",
    "\n",
    "#Separates decision function results into signal and background\n",
    "#along with training and testing\n",
    "osvmArrs.append(clf.decision_function(sigValsTrain).ravel())\n",
    "osvmArrs.append(clf.decision_function(sigValsTest).ravel())\n",
    "osvmArrs.append(clf.decision_function(backVals).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#%jsroot on9\n",
    "%matplotlib inline\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "#Sets up plot boundaries\n",
    "plotMin = min(min(osvmArrs[0]), min(osvmArrs[1]), min(osvmArrs[2]))\n",
    "plotMax = max(max(osvmArrs[0]), max(osvmArrs[1]), max(osvmArrs[2]))\n",
    "binz = np.linspace(plotMin, plotMax, 200)\n",
    "\n",
    "#Creates first histogram of Un-normalized Classification\n",
    "plt.figure(figsize=(7, 10))\n",
    "plt.subplot(211)\n",
    "plt.hist(osvmArrs[0], normed = False, bins = binz, edgecolor = 'red',   \n",
    "         facecolor = 'white', alpha=1, label = \"Signal Train\", linewidth = 1.5)\n",
    "plt.hist(osvmArrs[1], normed = False, bins = binz, edgecolor = 'green', \n",
    "         facecolor = 'white', alpha=1, label = \"Signal Test\")\n",
    "plt.hist(osvmArrs[2], normed = False, bins = binz, edgecolor = 'blue',  \n",
    "         facecolor = 'white', alpha=.5, label = \"Background\")\n",
    "plt.title(\"Classification Plot, OneClassSVM, Un-normalized, Nu = %s, Gamma = %s\" % (nuVal,gammaVal))\n",
    "plt.xlabel(\"Decision Function Score\")\n",
    "plt.ylabel(\"Counts per Bin\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "\n",
    "#Creates second histogram of Normalized Classification\n",
    "plt.subplot(212)\n",
    "plt.hist(osvmArrs[0], normed = True, bins = binz, edgecolor = 'red',   \n",
    "         facecolor = 'white', alpha=1, label = \"Signal Train\", linewidth = 1.5)\n",
    "plt.hist(osvmArrs[1], normed = True, bins = binz, edgecolor = 'green', \n",
    "         facecolor = 'white', alpha=1, label = \"Signal Test\")\n",
    "plt.hist(osvmArrs[2], normed = True, bins = binz, edgecolor = 'blue',  \n",
    "         facecolor = 'white', alpha=.5, label = \"Background\")\n",
    "plt.title(\"Classification Plot, OneClassSVM, Normalized, Nu = %s, Gamma = %s\" % (nuVal,gammaVal))\n",
    "plt.xlabel(\"Decision Function Score\")\n",
    "plt.ylabel(\"Counts per Bin\")\n",
    "plt.legend()\n",
    "\n",
    "#Prints relevant statistics below\n",
    "print(\"Loss Rate                                           : \", (falseNegTest/(truePosTest+falseNegTest)*100))\n",
    "print(\"Pollution Rate                                      : \", (falsePos/(truePosTest+falsePos))*100)\n",
    "print(\"Number of errors on training set : \", falseNegTrain, \" Percentage: \", (falseNegTrain/len(sigValsTrain)*100))\n",
    "print(\"Number of errors on test set     : \", falseNegTest, \" Percentage: \", (falseNegTest/len(sigValsTest)*100))\n",
    "print(\"Number of errors on outliers set : \", falsePos, \"  Percentage: \", (falsePos/len(backVals)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "print(labels.shape)\n",
    "# yTest = np.append(sigTargetTest, backTarget)\n",
    "yTest = kk\n",
    "# print(labels)\n",
    "# print(yTest)\n",
    "osvmScore = np.append(osvmArrs[1],osvmArrs[2])\n",
    "# osvmScore = np.append(y_pred_test , y_pred_outliers )\n",
    "# print(osvmScore)\n",
    "fpr, tpr, _ = roc_curve(kk, osvmScore)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2;\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "        lw = lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw = lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC OSVM, Tuned, Nu = %s, Gamma = %s' % (nuVal,gammaVal))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = np.zeros(len(data_n.values))\n",
    "for i, x in enumerate(data_n.values):\n",
    "    dist[i] = np.linalg.norm(x-decoded[i])\n",
    "    \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels, dist)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='red', label='AUC = %0.2f)' % roc_auc)\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.title('ROC Autoencoder 100-80-100 ReLU/Sigmoid synth\\_multidim\\_100\\_000')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
