{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 606 ms, sys: 58 ms, total: 664 ms\n",
      "Wall time: 988 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Imports requisite packages\n",
    "import os\n",
    "import time\n",
    "import numpy\n",
    "import pickle\n",
    "import cProfile\n",
    "import itertools\n",
    "import matplotlib\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.preprocessing\n",
    "import sklearn.learning_curve\n",
    "import sklearn.model_selection\n",
    "import sklearn.cross_validation\n",
    "import sklearn.feature_selection\n",
    "import sklearn.kernel_approximation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#%jsroot on9\n",
    "%matplotlib inline\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Takes the converted tree and turns it into an\n",
    "#n-by-30 array usable by sklearn.\n",
    "def outputs(array):\n",
    "    #Only uses events with non-zero luminosity\n",
    "    goodEvents = array[array['lumi'] != 0]\n",
    "    ind = numpy.lexsort((goodEvents['lumiId'],goodEvents['runId']))\n",
    "    events = goodEvents[ind]\n",
    "    dataset = numpy.empty([len(goodEvents),30])\n",
    "    target = numpy.empty([len(goodEvents)])\n",
    "    badOnes = numpy.array([])\n",
    "\n",
    "    #Fills dataset array with proper features\n",
    "    for j, event in enumerate(events):\n",
    "        try:\n",
    "            dataset[j,0:7] = event['qPFJetPt']\n",
    "            dataset[j,7:14] = event['qPFJetEta']\n",
    "            dataset[j,14:21] = event['qPFJetPhi']\n",
    "            dataset[j,21:28] = event['qNVtx']\n",
    "            dataset[j,28] = event['crossSection']\n",
    "            dataset[j,29] = event['lumi']\n",
    "            target[j] = event['isSig']\n",
    "        except ValueError:\n",
    "            badOnes = numpy.append(badOnes,j)\n",
    "            \n",
    "    #Takes out corrupt events\n",
    "    mask = numpy.zeros(len(dataset), dtype=bool)\n",
    "    mask[badOnes.astype(int)] = True\n",
    "    mask = ~mask\n",
    "    dataset = dataset[mask]\n",
    "    target = target[mask]\n",
    "       \n",
    "    return dataset, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializes and fits SVM classifier\n",
    "def suppVM(xTrain, yTrain, lumiTrain):\n",
    "    start = time.time()\n",
    "    classifier = SVC(C = 100, kernel = 'rbf',tol=0.0001,gamma='auto') \n",
    "    \n",
    "    #Comment sample_weight if the training is to be done without sample weight.\n",
    "    if weights == True:\n",
    "        classifier = classifier.fit(xTrain, yTrain, sample_weight = lumiTrain) \n",
    "    else:\n",
    "        classifier = classifier.fit(xTrain, yTrain)\n",
    "    \n",
    "    print('It took', time.time()-start, 'seconds for SVM.')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function that plots confusion matrix, taken from sklearn website\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    #This function prints and plots the confusion matrix.\n",
    "    #Normalization can be applied by setting `normalize=True`.     \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = numpy.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = numpy.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max()*.7\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Settings and Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pick learning algorithm, crossSection, weights, and random splitting\n",
    "SVM = True\n",
    "crossSection = True\n",
    "weights = False\n",
    "random = True\n",
    "odds = False\n",
    "\n",
    "#Saves dataset for pickling\n",
    "# dataset, target = outputs(array)\n",
    "# outFile = open('data.pkl', 'wb')\n",
    "# pickle.dump(dataset, outFile)\n",
    "# pickle.dump(target, outFile)\n",
    "# outFile.close()\n",
    "\n",
    "#Loads pickled dataset\n",
    "inFile = open('realData.pkl', 'rb')\n",
    "dataset = pickle.load(inFile, encoding=\"latin1\")\n",
    "target = pickle.load(inFile, encoding=\"latin1\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splits dataset into training and testing sets based on settings\n",
    "\n",
    "# xTrain = []\n",
    "# yTrain = []\n",
    "# xTest = []\n",
    "# yTest = []\n",
    "\n",
    "# if random == True:\n",
    "#     xTrain, xTest, yTrain, yTest = train_test_split(dataset, target, test_size=0.5)\n",
    "      \n",
    "      #The \n",
    "#     lumiTrain = xTrain[:,-1]\n",
    "#     lumiTest = xTest[:,-1]\n",
    "#     lumiTrain = numpy.copy(lumiTrain, order='C')\n",
    "#     lumiTest = numpy.copy(lumiTest, order='C')\n",
    "      \n",
    "#     if crossSection == True:\n",
    "#         xTrain = xTrain[:,:-1]\n",
    "#         xTest = xTest[:,:-1]\n",
    "#     else:\n",
    "#         xTrain = xTrain[:,:-2]\n",
    "#         xTest = xTest[:,:-2]\n",
    "# elif odds == True:\n",
    "#     lumiTrain = dataset[::2,-1]\n",
    "#     lumiTest = dataset[1::2,-1]\n",
    "#     lumiTrain = numpy.copy(lumiTrain, order='C')\n",
    "#     lumiTest = numpy.copy(lumiTest, order='C')\n",
    "    \n",
    "#     dataset = dataset[:,:-1]\n",
    "    \n",
    "#     xTrain = dataset[::2]\n",
    "#     xTest = dataset[1::2]\n",
    "#     yTrain = target[::2]\n",
    "#     yTest = target[1::2]\n",
    "# else:\n",
    "#     n = int(numpy.floor(len(dataset)/2))\n",
    "#     lumiTrain = dataset[:n,-1]\n",
    "#     lumiTest = dataset[n:,-1]\n",
    "#     lumiTrain = numpy.copy(lumiTrain, order='C')\n",
    "#     lumiTest = numpy.copy(lumiTest, order='C')\n",
    "    \n",
    "#     if crossSection ==  True:\n",
    "#         xTrain = dataset[:n,:-1]\n",
    "#         xTest = dataset[n:,:-1]\n",
    "#     else:\n",
    "#         xTrain = dataset[:n,:-2]\n",
    "#         xTest = dataset[n:,:-2]\n",
    "        \n",
    "#     yTrain = target[:n]\n",
    "#     yTest = target[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports training and testing sets used across models\n",
    "inFile = open('splits.pkl', 'rb')\n",
    "xTrain = pickle.load(inFile, encoding=\"latin1\")\n",
    "xTest = pickle.load(inFile, encoding=\"latin1\")\n",
    "yTrain = pickle.load(inFile, encoding=\"latin1\")\n",
    "yTest = pickle.load(inFile, encoding=\"latin1\")\n",
    "lumiTrain = pickle.load(inFile, encoding=\"latin1\")\n",
    "lumiTest = pickle.load(inFile, encoding=\"latin1\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Removes features with variance less than 0.1\n",
    "# sel = VarianceThreshold(threshold = 0.01)\n",
    "# print(xTrain.shape)\n",
    "# sel.fit(xTrain)\n",
    "# indices = sel.get_support()\n",
    "# xTrain = xTrain[:,indices]\n",
    "# print(xTrain.shape)\n",
    "# xTest = xTest[:,indices]\n",
    "# print(indices)\n",
    "\n",
    "#Removes features based on different metrics\n",
    "\n",
    "# print(xTrain.shape)\n",
    "# genSel = GenericUnivariateSelect(chi2, mode = 'k_best', param = 5)\n",
    "# genSel.fit(xTrain,yTrain)\n",
    "# indices = genSel.get_support()\n",
    "# print(indices)\n",
    "# xTrain = xTrain[:,indices]\n",
    "# xTest = xTest[:,indices]\n",
    "# print(xTrain.shape)\n",
    "# print(xTest.shape)\n",
    "# mask = numpy.ones(29,dtype=bool)\n",
    "# print(mask)\n",
    "# ind = [1, 2, 6, 7, 9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# mask[ind] = False\n",
    "# print(mask)\n",
    "# print(xTrain.shape)\n",
    "# xTrain = xTrain[:,mask]\n",
    "# print(xTrain.shape)\n",
    "# xTest = xTest[:,mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0  1  4  5  6 21 22 23 24 25 26 27 28]\n",
    "[2 3 7 8 9 10 11 12 13 14 15 16 17 18 19 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-78007a237803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Scales the data to zero mean and unit variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "#Scales the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xTrain)\n",
    "xTrain = scaler.transform(xTrain)\n",
    "xTest = scaler.transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Grid Search with cross validation\n",
    "\n",
    "tunedParams = [{'gamma': [.001, 0.0001, .5, 1],}]\n",
    "scores = ['precision', 'recall', 'roc_auc', 'f1']\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    clf = GridSearchCV(SVC(kernel='rbf', C = 100, tol = 0.001), tunedParams, scoring='%s' % score, n_jobs=-1)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = yTest, clf.predict(xTest)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Kernel approximation\n",
    "\n",
    "# rbfFeature = RBFSampler(gamma = .0001, n_components = 10000)\n",
    "# rbfFeature.fit(xTrain)\n",
    "# newXTrain = rbfFeature.transform(xTrain)\n",
    "# newXTest = rbfFeature.transform(xTest)\n",
    "# nysFeature = Nystroem(gamma = .001, n_components = 1000)\n",
    "# nysFeature.fit(xTrain)\n",
    "# newXTrain = nysFeature.transform(xTrain)\n",
    "# newXTest = nysFeature.transform(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Trains and Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calls classifier function\n",
    "svmClf = suppVM(xTrain, yTrain, lumiTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pickles classifier\n",
    "\n",
    "# outClfs = open('svmClf_tuned_weights.pkl', 'wb')\n",
    "# pickle.dump(svmClf, outClfs)\n",
    "# outClfs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unpickles classifier\n",
    "\n",
    "# outClfs = open('svmClf_untuned_weights.pkl', 'rb')\n",
    "# svmClf = pickle.load(outClfs)\n",
    "# outClfs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Provides classification reports \n",
    "if weights == True:    \n",
    "    svmScore = svmClf.fit(xTrain, yTrain, sample_weight = lumiTrain).decision_function(xTest)\n",
    "else:\n",
    "    svmScore = svmClf.fit(xTrain, yTrain).decision_function(xTest)\n",
    "    \n",
    "svmPredict = svmClf.predict(xTest)\n",
    "print(\"Classification report for SVM, Tuned, Weights %s:\\n%s\\n\"\n",
    "      % (svmClf, metrics.classification_report(yTest, svmPredict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pickles scores\n",
    "\n",
    "# outfile = open('svm_classscores_tuned_weights.pkl', 'wb')\n",
    "# pickle.dump(svmScore, outfile)\n",
    "# pickle.dump(svmPredict, outfile)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unpickles scores\n",
    "\n",
    "# inFile = open('svm_classscores_untuned_noweights', 'rb')\n",
    "# svmScore = pickle.load(inFile)\n",
    "# svmPredict = pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots classification results for signal and background\n",
    "svmArrs = []\n",
    "svmHists = []\n",
    "\n",
    "#Separates decision function results into signal and background\n",
    "#along with training and testing\n",
    "svmArrs.append(svmClf.decision_function(xTrain[yTrain>0.5]).ravel())\n",
    "svmArrs.append(svmClf.decision_function(xTrain[yTrain<0.5]).ravel())\n",
    "svmArrs.append(svmClf.decision_function(xTest[yTest>0.5]).ravel())\n",
    "svmArrs.append(svmClf.decision_function(xTest[yTest<0.5]).ravel())\n",
    "\n",
    "#Turns those arrays into histograms\n",
    "svmHists.append(list(numpy.histogram(svmArrs[0], normed = True, bins = 40)))\n",
    "svmHists.append(list(numpy.histogram(svmArrs[1], normed = True, bins = 40)))\n",
    "svmHists.append(list(numpy.histogram(svmArrs[2], normed = True, bins = 40)))\n",
    "svmHists.append(list(numpy.histogram(svmArrs[3], normed = True, bins = 40)))\n",
    "\n",
    "#Defines bin edges, centers, and widths\n",
    "svmMax = max([hist[0].max() for hist in svmHists])*1.2\n",
    "svmMin = max([hist[0].min() for hist in svmHists])\n",
    "svmEdges = svmHists[0][1]\n",
    "svmCenters = (svmEdges[:-1] + svmEdges[1:])/2.\n",
    "svmWidths = (svmEdges[1:] - svmEdges[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Normalizes histogram based on maximum value\n",
    "svmNormVal1 = max(max(svmHists[0][0]), max(svmHists[1][0]))\n",
    "svmNormVal2 = max(max(svmHists[2][0]), max(svmHists[3][0]))\n",
    "svmHists[0][0] = [x/svmNormVal1 for x in svmHists[0][0]]\n",
    "svmHists[1][0] = [x/svmNormVal1 for x in svmHists[1][0]]\n",
    "svmHists[2][0] = [x/svmNormVal2 for x in svmHists[2][0]]\n",
    "svmHists[3][0] = [x/svmNormVal2 for x in svmHists[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots histograms\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.bar(svmCenters-svmWidths/2.,svmHists[0][0],facecolor='red',linewidth=0,width=svmWidths,label='Signal',alpha=0.5)\n",
    "ax1.bar(svmCenters-svmWidths/2.,svmHists[1][0],facecolor='blue',linewidth=0,width=svmWidths,label='Background',alpha=0.5)\n",
    "#Change depending on which classifier and options are chosen\n",
    "plt.title(\"Classification, SVM, Tuned, Weights, Rand, 15 feats, Training Set\")\n",
    "plt.xlabel(\"classifier score\")\n",
    "plt.ylabel(\"Counts/Bin\")\n",
    "legend = ax1.legend(loc='upper center', shadow=True,ncol=2)\n",
    "for alabel in legend.get_texts():\n",
    "            alabel.set_fontsize('small')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "ax2 = plt.subplot(111)\n",
    "ax2.bar(svmCenters-svmWidths/2.,svmHists[2][0],facecolor='red',linewidth=0,width=svmWidths,label='Signal',alpha=0.5)\n",
    "ax2.bar(svmCenters-svmWidths/2.,svmHists[3][0],facecolor='blue',linewidth=0,width=svmWidths,label='Background',alpha=0.5)\n",
    "plt.title(\"Classification, SVM, Tuned, Weights, Rand, 15 feats, Testing Set\")\n",
    "plt.xlabel(\"classifier score\")\n",
    "plt.ylabel(\"Counts/Bin\")\n",
    "legend = ax1.legend(loc='upper center', shadow=True,ncol=2)\n",
    "for alabel in legend.get_texts():\n",
    "            alabel.set_fontsize('small')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots roc curve, code taken from sklearn website\n",
    "fpr, tpr, _ = roc_curve(yTest, svmScore)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2;\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "        lw = lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw = lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC SVM, Tuned, Weights, Rand, 15 feat')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots confusion matrix, code taken from sklearn\n",
    "classNames = ['Background','Signal']\n",
    "confMat = confusion_matrix(yTest, svmPredict)\n",
    "#numpy.set_printoptions(precision=)\n",
    "\n",
    "#Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confMat, classes=classNames,\n",
    "                      title='Confusion matrix, SVM, Tuned, Un-normalization, Weights')\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confMat, classes=classNames, normalize=True,\n",
    "                      title='Confusion Matrix, SVM, Tuned, Normalized, Weights')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calculates Matthews Correlation Coefficient\n",
    "#Ranges from -1 to 1, with 1 being a perfect predictor\n",
    "matthews_corrcoef(yTest,svmPredict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
