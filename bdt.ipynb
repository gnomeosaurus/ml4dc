{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 637 ms, sys: 64 ms, total: 701 ms\n",
      "Wall time: 1.29 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Imports requisute packages\n",
    "import os\n",
    "import time\n",
    "import numpy\n",
    "import pickle\n",
    "import cProfile\n",
    "import itertools\n",
    "import matplotlib\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.preprocessing\n",
    "import sklearn.learning_curve\n",
    "import sklearn.model_selection\n",
    "import sklearn.cross_validation\n",
    "import sklearn.feature_selection\n",
    "import sklearn.kernel_approximation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#%jsroot on9\n",
    "%matplotlib inline\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converts ROOT tree to numpy array\n",
    "\n",
    "# %%time\n",
    "# tChain = rt.TChain('MyAnalysis/MyTree')\n",
    "# tChain.Add(\"ntuples/*.root\")\n",
    "# array = root_numpy.tree2array(tChain)\n",
    "# print 'Total number of entries: ',tChain.GetEntries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 17.6 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Takes the converted tree and turns it into an\n",
    "#n-by-30 array usable by sklearn.\n",
    "def outputs(array):\n",
    "    #Only uses events with non-zero luminosity\n",
    "    goodEvents = array[array['lumi'] != 0]\n",
    "    ind = numpy.lexsort((goodEvents['lumiId'],goodEvents['runId']))\n",
    "    events = goodEvents[ind]\n",
    "    dataset = numpy.empty([len(goodEvents),30])\n",
    "    target = numpy.empty([len(goodEvents)])\n",
    "    badOnes = numpy.array([])\n",
    "\n",
    "    #Fills dataset array with proper features\n",
    "    for j, event in enumerate(events):\n",
    "        try:\n",
    "            dataset[j,0:7] = event['qPFJetPt']\n",
    "            dataset[j,7:14] = event['qPFJetEta']\n",
    "            dataset[j,14:21] = event['qPFJetPhi']\n",
    "            dataset[j,21:28] = event['qNVtx']\n",
    "            dataset[j,28] = event['crossSection']\n",
    "            dataset[j,29] = event['lumi']\n",
    "            target[j] = event['isSig']\n",
    "        except ValueError:\n",
    "            badOnes = numpy.append(badOnes,j)\n",
    "          \n",
    "    #Takes out corrupt events\n",
    "    mask = numpy.zeros(len(dataset), dtype=bool)\n",
    "    mask[badOnes.astype(int)] = True\n",
    "    mask = ~mask\n",
    "    dataset = dataset[mask]\n",
    "    target = target[mask]\n",
    "       \n",
    "    return dataset, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializes and fits BDT classifier\n",
    "def dTree(xTrain, yTrain, lumiTrain):\n",
    "    start = time.time()\n",
    "    classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 50, \n",
    "                                    min_samples_split = 50, min_samples_leaf = 1),\n",
    "                                    algorithm=\"SAMME.R\", n_estimators=200,\n",
    "                                    learning_rate=1)\n",
    "# Comment sample_weight if the training is to be done without sample weight.),\n",
    "    if weights == True:\n",
    "        classifier = classifier.fit(xTrain, yTrain, sample_weight = lumiTrain) \n",
    "    else:\n",
    "        classifier = classifier.fit(xTrain, yTrain)\n",
    "\n",
    "    print('It took', time.time()-start, 'seconds for BDT.')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function that plots confusion matrix, taken from sklearn website\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    #This function prints and plots the confusion matrix.\n",
    "    #Normalization can be applied by setting `normalize=True`.     \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = numpy.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = numpy.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max()*.7\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Settings and Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pick learning algorithm, crossSection, weights, and random splitting\n",
    "BDT = True\n",
    "crossSection = True\n",
    "weights = False\n",
    "random = True\n",
    "odds = False\n",
    "\n",
    "#Saves dataset for pickling\n",
    "# dataset, target = outputs(array)\n",
    "# outFile = open('data.pkl', 'wb')\n",
    "# pickle.dump(dataset, outFile)\n",
    "# pickle.dump(target, outFile)\n",
    "# outFile.close()\n",
    "\n",
    "#Loads pickled dataset\n",
    "inFile = open('realData.pkl', 'rb')\n",
    "dataset = pickle.load(inFile, encoding=\"latin1\")\n",
    "target = pickle.load(inFile, encoding=\"latin1\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6fd00bbd63f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlumiTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#Splits dataset into training and testing sets based on settings\n",
    "xTrain = []\n",
    "yTrain = []\n",
    "xTest = []\n",
    "yTest = []\n",
    "\n",
    "if random == True:\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(dataset, target, test_size=0.5)\n",
    "\n",
    "    lumiTrain = xTrain[:,-1]\n",
    "    lumiTest = xTest[:,-1]\n",
    "    lumiTrain = numpy.copy(lumiTrain, order='C')\n",
    "    lumiTest = numpy.copy(lumiTest, order='C')\n",
    "\n",
    "    if crossSection == True:\n",
    "        xTrain = xTrain[:,:-1]\n",
    "        xTest = xTest[:,:-1]\n",
    "    else:\n",
    "        xTrain = xTrain[:,:-2]\n",
    "        xTest = xTest[:,:-2]\n",
    "elif odds == True:\n",
    "    lumiTrain = dataset[::2,-1]\n",
    "    lumiTest = dataset[1::2,-1]\n",
    "    lumiTrain = numpy.copy(lumiTrain, order='C')\n",
    "    lumiTest = numpy.copy(lumiTest, order='C')\n",
    "    \n",
    "    xTrain = dataset[::2]\n",
    "    xTest = dataset[1::2]\n",
    "    yTrain = target[::2]\n",
    "    yTest = target[1::2]\n",
    "else:\n",
    "    n = int(numpy.floor(len(dataset)/5))\n",
    "    lumiTrain = dataset[:n*4,-1]\n",
    "    lumiTest = dataset[n*4:,-1]\n",
    "    lumiTrain = numpy.copy(lumiTrain, order='C')\n",
    "    lumiTest = numpy.copy(lumiTest, order='C')\n",
    "    \n",
    "    if crossSection ==  True:\n",
    "        xTrain = dataset[:n*4,:-1]\n",
    "        xTest = dataset[n*4:,:-1]\n",
    "    else:\n",
    "        xTrain = dataset[:n*4,:-2]\n",
    "        xTest = dataset[n*4:,:-2]\n",
    "        \n",
    "    yTrain = target[:n*4]\n",
    "    yTest = target[n*4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports training and testing sets used across models\n",
    "inFile = open('splits.pkl', 'rb')\n",
    "xTrain = pickle.load(inFile, encoding=\"latin1\")\n",
    "xTest = pickle.load(inFile, encoding=\"latin1\")\n",
    "yTrain = pickle.load(inFile, encoding=\"latin1\")\n",
    "yTest = pickle.load(inFile, encoding=\"latin1\")\n",
    "lumiTrain = pickle.load(inFile, encoding=\"latin1\")\n",
    "lumiTest = pickle.load(inFile, encoding=\"latin1\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removes features with variance less than 0.1\n",
    "sel = VarianceThreshold(threshold = 0.01)\n",
    "print(xTrain.shape)\n",
    "sel.fit(xTrain)\n",
    "indices = sel.get_support()\n",
    "xTrain = xTrain[:,indices]\n",
    "print(xTrain.shape)\n",
    "xTest = xTest[:,indices]\n",
    "print(indices)\n",
    "\n",
    "#Removes features based on different metrics\n",
    "\n",
    "# print(xTrain.shape)\n",
    "# genSel = GenericUnivariateSelect(chi2, mode = 'k_best', param = 5)\n",
    "# genSel.fit(xTrain,yTrain)\n",
    "# indices = genSel.get_support()\n",
    "# print(indices)\n",
    "# xTrain = xTrain[:,indices]\n",
    "# xTest = xTest[:,indices]\n",
    "# print(xTrain.shape)\n",
    "# print(xTest.shape)\n",
    "# mask = numpy.ones(29,dtype=bool)\n",
    "# print(mask)\n",
    "# ind = [1, 2, 6, 7, 9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# mask[ind] = False\n",
    "# print(mask)\n",
    "# print(xTrain.shape)\n",
    "# xTrain = xTrain[:,mask]\n",
    "# print(xTrain.shape)\n",
    "# xTest = xTest[:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scales the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xTrain)\n",
    "xTrain = scaler.transform(xTrain)\n",
    "xTest = scaler.transform(xTest)\n",
    "print(len(yTrain[yTrain==0]))\n",
    "print(len(yTrain[yTrain==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid Search with cross validation\n",
    "\n",
    "tunedParams = [{'algorithm': ['SAMME', 'SAMME.R']}]\n",
    "scores = ['precision', 'recall', 'roc_auc', 'f1']\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(max_depth = 50, \n",
    "                       min_samples_split = 50, min_samples_leaf = 10)),\n",
    "                       tunedParams, cv=3, n_jobs=-1, scoring='%s' % score)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = yTest, clf.predict(xTest)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Kernel approximation\n",
    "\n",
    "# rbfFeature = RBFSampler(gamma = 10, n_components = 10000)\n",
    "# rbfFeature.fit(xTrain)\n",
    "# newXTrain = rbfFeature.transform(xTrain)\n",
    "# newXTest = rbfFeature.transform(xTest)\n",
    "# nysFeature = Nystroem(gamma = 100, n_components = 1000)\n",
    "# nysFeature.fit(xTrain)\n",
    "# newXTrain = nysFeature.transform(xTrain)\n",
    "# newXTest = nysFeature.transform(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Trains and Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calls classifier function\n",
    "bdtClf = dTree(xTrain, yTrain, lumiTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pickles classifier\n",
    "\n",
    "# outClfs = open('bdtClf_tuned_weights.pkl', 'wb')\n",
    "# pickle.dump(bdtClf, outClfs)\n",
    "# outClfs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unpickles classifier\n",
    "\n",
    "# outClfs = open('svmClf_untuned_noweights.pkl', 'rb')\n",
    "# svmClf = pickle.load(outClfs)\n",
    "# outClfs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Provides classification reports \n",
    "if weights == True:    \n",
    "    bdtScore = bdtClf.fit(xTrain, yTrain, sample_weight = lumiTrain).decision_function(xTest)\n",
    "else:\n",
    "    bdtScore = bdtClf.fit(xTrain,yTrain).decision_function(xTest)\n",
    "    \n",
    "bdtPredict = bdtClf.predict(xTest)\n",
    "print(\"Classification report for BDT, Tuned, Weights %s:\\n%s\\n\"\n",
    "      % (bdtClf, metrics.classification_report(yTest, bdtPredict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pickles scores\n",
    "\n",
    "# outfile = open('bdt_classreport_tuned_weights.pkl', 'wb')\n",
    "# pickle.dump(bdtScore, outfile)\n",
    "# pickle.dump(bdtPredict, outfile)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unpickles scores\n",
    "\n",
    "# inFile = open('classScores_No_Cross_No_Weights_OddsEvens.pkl', 'rb')\n",
    "# svmScore = pickle.load(inFile)\n",
    "# bdtScore = pickle.load(inFile)\n",
    "# svmPredictor = pickle.load(inFile)\n",
    "# bdtPredictor = pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots classification results for signal and background\n",
    "bdtArrs = []\n",
    "bdtHists = []\n",
    "\n",
    "#Separates decision function results into signal and background\n",
    "#along with training and testing\n",
    "bdtArrs.append(bdtClf.decision_function(xTrain[yTrain>0.5]).ravel())\n",
    "bdtArrs.append(bdtClf.decision_function(xTrain[yTrain<0.5]).ravel())\n",
    "bdtArrs.append(bdtClf.decision_function(xTest[yTest>0.5]).ravel())\n",
    "bdtArrs.append(bdtClf.decision_function(xTest[yTest<0.5]).ravel())\n",
    "\n",
    "#Turns those arrays into histograms\n",
    "bdtHists.append(list(numpy.histogram(bdtArrs[0], normed = True, bins = 40)))\n",
    "bdtHists.append(list(numpy.histogram(bdtArrs[1], normed = True, bins = 40)))\n",
    "bdtHists.append(list(numpy.histogram(bdtArrs[2], normed = True, bins = 40)))\n",
    "bdtHists.append(list(numpy.histogram(bdtArrs[3], normed = True, bins = 40)))\n",
    "\n",
    "#Defines bin edges, centers, and widths\n",
    "bdtMax = max([hist[0].max() for hist in bdtHists])*1.2\n",
    "bdtMin = max([hist[0].min() for hist in bdtHists])\n",
    "bdtEdges = bdtHists[0][1]\n",
    "bdtCenters = (bdtEdges[:-1] + bdtEdges[1:])/2.\n",
    "bdtWidths = (bdtEdges[1:] - bdtEdges[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Normalizes histogram based on maximum value\n",
    "bdtNormVal1 = max(max(bdtHists[0][0]), max(bdtHists[1][0]))\n",
    "bdtNormVal2 = max(max(bdtHists[2][0]), max(bdtHists[3][0]))\n",
    "bdtHists[0][0] = [x/bdtNormVal1 for x in bdtHists[0][0]]\n",
    "bdtHists[1][0] = [x/bdtNormVal1 for x in bdtHists[1][0]]\n",
    "bdtHists[2][0] = [x/bdtNormVal2 for x in bdtHists[2][0]]\n",
    "bdtHists[3][0] = [x/bdtNormVal2 for x in bdtHists[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots histograms\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.bar(bdtCenters-bdtWidths/2.,bdtHists[0][0],facecolor='red',linewidth=0,width=bdtWidths,label='Signal',alpha=0.5)\n",
    "ax1.bar(bdtCenters-bdtWidths/2.,bdtHists[1][0],facecolor='blue',linewidth=0,width=bdtWidths,label='Background',alpha=0.5)\n",
    "#Change depending on which classifier and options are chosen\n",
    "plt.title(\"Classification, BDT, Tuned, Weights, Rand, 15 feats, Training Set\")\n",
    "plt.xlabel(\"classifier score\")\n",
    "plt.ylabel(\"Counts/Bin\")\n",
    "legend = ax1.legend(loc='upper center', shadow=True,ncol=2)\n",
    "for alabel in legend.get_texts():\n",
    "            alabel.set_fontsize('small')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    " \n",
    "ax2 = plt.subplot(111)\n",
    "ax2.bar(bdtCenters-bdtWidths/2.,bdtHists[2][0],facecolor='red',linewidth=0,width=bdtWidths,label='Signal',alpha=0.5)\n",
    "ax2.bar(bdtCenters-bdtWidths/2.,bdtHists[3][0],facecolor='blue',linewidth=0,width=bdtWidths,label='Background',alpha=0.5)\n",
    "plt.title(\"Classification, BDT, Tuned, Weights, Rand, 15 feats, Testing Set\")\n",
    "plt.xlabel(\"classifier score\")\n",
    "plt.ylabel(\"Counts/Bin\")\n",
    "legend = ax1.legend(loc='upper center', shadow=True,ncol=2)\n",
    "for alabel in legend.get_texts():\n",
    "            alabel.set_fontsize('small')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots roc curve, code taken from sklearn website\n",
    "fpr, tpr, _ = roc_curve(yTest, bdtScore)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2;\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "        lw = lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw = lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC BDT, Tuned, Weights, Rand, 15 feats')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plots confusion matrix, code taken from sklearn\n",
    "classNames = ['Background','Signal']\n",
    "confMat = confusion_matrix(yTest, bdtPredict)\n",
    "\n",
    "#Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confMat, classes=classNames,\n",
    "                      title='Confusion matrix, BDT, Tuned, Un-normalization, Weights')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confMat, classes=classNames, normalize=True,\n",
    "                      title='Confusion Matrix, BDT, Tuned, Normalized, Weights')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calculates Matthews Correlation Coefficient\n",
    "#Ranges from -1 to 1, with 1 being a perfect predictor\n",
    "matthews_corrcoef(yTest,bdtPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
